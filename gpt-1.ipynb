{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26263029-5066-42cd-8290-68e03cd4c359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random \n",
    "import pickle\n",
    "import argparse\n",
    "'''\n",
    "parser = argparse.ArgumentParser(description='Demo')\n",
    "\n",
    "parser.add_argument('-batch_size', type=str, required=True, help='Please provide a batch_size')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f'batch size: {args.batch_size}')\n",
    "'''\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# hyper params \n",
    "block_size = 64\n",
    "batch_size = 128\n",
    "\n",
    "max_iters = 3000\n",
    "# eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "#eval_interval = 500\n",
    "dropout = 0.2 # prevents over fitting \n",
    "n_embd = 384 # may be too big for PC --> this creates a vector for each word/char about its relevence\n",
    "# take sad and happy sad may be [0.1, 0.8] say the first index is the positivity of the word\n",
    "# and the second index is if its showing some sort of emotion, this helps us classify words ish\n",
    "n_layer = 8 # each of these higher elarns more\n",
    "n_head = 8\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e77b48-6a8c-4c02-9f30-d2731240549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ''\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f: # this is our vocab\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7361ec72-94c9-497f-8f1f-59275af04ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0e927d-cf22-4a2c-a1aa-7d60a698e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else \"val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "   # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c751365-9e7d-46b0-9854-e5a1642d9e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@torch.no_grad()\\ndef estimate_loss():\\n    out = {}\\n    model.eval()\\n    for split in ['train', 'val']:\\n        losses = torch.zeros(eval_iters)\\n        for k in range(eval_iters):\\n            X, Y = get_batch(split)\\n            logits, loss = model(X, Y)\\n            losses[k] = loss.item()\\n        out[split] = losses.mean()\\n    model.train()\\n    return out\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6010237-effe-4fd9-b1b4-7dd192942abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\programming\\cuda\\lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # inpout of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B,T,hs) @ (B, hs, T) -> (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd) # adds in another learnable param\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concats each head toghether along the last dimetion (B,T,F) -> (B,T,[h1, h1, h1, h1, h2, h2, h2, h2, ...])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__ (self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(), # looks at a number and if <0 num =0 else num stays the same \n",
    "            nn.Linear(4* n_embd, n_embd), \n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x+y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x+y)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embeding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positonal_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # howmany decoder blocks\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(n_embd) # can go and experiment with differnt norms\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # makes it softMax workable\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        ''' This is for the behind the scenes shit helping us understand what is going on under the hood --> also a lot easier to debug\n",
    "        What are the logits:\n",
    "        a bunch of normaized floating point numbers\n",
    "        we sum the numbers and then div each number in the set buy the total (normalization) -> this gives us a prob dist of what we want to predict\n",
    "        '''  \n",
    "        B, T = index.shape\n",
    "\n",
    "        tok_emb = self.token_embeding_table(index)\n",
    "        pos_emb = self.positonal_embedding_table(torch.arange(T, device=device)) # T, C\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, Vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "    # What does view do: al;lwos us to unpack with.shapoe and then pack back totgether\n",
    "            B, T, C = logits.shape # Batch, Time, Channels\n",
    "            logits = logits.view(B*T, C) #The batch and time arnt suepr important so we can blend them together\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # we use veiw to ensure that the shape that this funct expects is met -> it exprect b by c bt t \n",
    "            \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes b, c\n",
    "            # apply softmax tro get probs\n",
    "            probs = F.softmax(logits, dim=-1) # b,c\n",
    "            #sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # b, 1\n",
    "            # append the sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # B, T+1\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "print('loading model parameters...')\n",
    "with open('model-01.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "print('loaded successfully!')\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c3ae86-7fd7-4484-9f5e-102d9c5e4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d53862-94ac-48ee-a9da-2bd48baa4daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 1.420, val loss: 1.453\n",
      "step: 100, train loss: 1.418, val loss: 1.469\n",
      "step: 200, train loss: 1.418, val loss: 1.462\n",
      "step: 300, train loss: 1.430, val loss: 1.434\n",
      "step: 400, train loss: 1.417, val loss: 1.394\n",
      "step: 500, train loss: 1.409, val loss: 1.399\n",
      "step: 600, train loss: 1.449, val loss: 1.373\n",
      "step: 700, train loss: 1.357, val loss: 1.404\n",
      "step: 800, train loss: 1.413, val loss: 1.408\n",
      "step: 900, train loss: 1.501, val loss: 1.411\n",
      "step: 1000, train loss: 1.371, val loss: 1.377\n",
      "step: 1100, train loss: 1.369, val loss: 1.388\n",
      "step: 1200, train loss: 1.413, val loss: 1.474\n",
      "step: 1300, train loss: 1.438, val loss: 1.354\n",
      "step: 1400, train loss: 1.367, val loss: 1.409\n",
      "step: 1500, train loss: 1.358, val loss: 1.352\n",
      "step: 1600, train loss: 1.393, val loss: 1.376\n",
      "step: 1700, train loss: 1.400, val loss: 1.390\n",
      "step: 1800, train loss: 1.420, val loss: 1.314\n",
      "step: 1900, train loss: 1.415, val loss: 1.358\n",
      "step: 2000, train loss: 1.363, val loss: 1.400\n",
      "step: 2100, train loss: 1.364, val loss: 1.286\n",
      "step: 2200, train loss: 1.314, val loss: 1.334\n",
      "step: 2300, train loss: 1.327, val loss: 1.400\n",
      "step: 2400, train loss: 1.328, val loss: 1.297\n",
      "step: 2500, train loss: 1.336, val loss: 1.408\n",
      "step: 2600, train loss: 1.395, val loss: 1.418\n",
      "step: 2700, train loss: 1.314, val loss: 1.336\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # print(iter)\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010423e5-2084-4f14-80e7-f7e1973c2f71",
   "metadata": {},
   "source": [
    "## context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-llm",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
